# Common Data Problems

## Why clean data?

In a typical data science workflow, we will:

1. access raw data 
2. explore and process data
3. extract insights using visualisations or predicted models
4. report insights with dashboards or reports

Dirty data can appear because of duplicate values, misspellings, data type parsing errors
and legacy systems. (There are more reasons, really.)

If we do not make sure that data is clean before our workflow starts, it will definitely
affect the eventual final product that we produce (ie the insights).

## Data Type Constraints

There are many types of data we work with! This table shows the more common data types
used.

| Datatypes | Python Data Types |
|---|---|
| Text data | `str` |
| Integers | `int` |
| Decimals | `float` |
| Binary | `bool` |
| Date | `datetime` |
| Categories | `category` |

To make sure our analyses make sense, we need to make sure our data are represented
by the correct datatypes.

### Example: Strings to Integers

For example, say we have a csv file that contains the revenue and quantity of each
sales order, `sales.csv`.

We convert it into a DataFrame and output the head:

```python
  import python as pd
  sales = pd.read_csv("sales.csv")
  sales.head(2)
```

```console
    SalesOrderID    Revenue    Quantity
  0        43659     23153$          12
  1        43660      1457$           2
```

We want to calculate the total revenue generated by all sales orders.

We can see that each entry in the column `Revenue` has `$` on the right hand side.
This suggests that entries under `Revenue` are likely strings.

To further confirm our suspicions, we use the `.dtypes` method to inspect the data types
of each column.

```python
  sales.dtypes
```

```console
  SalesOrderID    int64
  Revenue         object
  Quantity        int64
  dtype: object
```

`object` - used to store strings in Pandas - is returned for the `Revenue` column,
confirming our suspicions!

> Alternatively, we could have used the `.info()` method too. This retrieves
> DataFrame information, which includes not just the data types, but also the number
> of missing values in the data.

If we summed up all entries in the `Revenue` column directly, it will result in
a concatenation of all the texts since they are all strings!

```python
  sales["Revenue"].sum()
```

```console
'23153$1457$36865$...
```

Thus, we have to first process the data.

We do this by removing `$` at the end of each entry using the `.strip()` method and
pass in the string we want to strip as the argument (in this case `$`).

Then, we typecast each entry from `str` to `int` using the `.astype()` method. Within the
method, we specify our desired type as the argument. This means it could have easily been
`"float"` being passed in, and each entry will be typecasted into decimal values instead
of integers.

```python
  sales["Revenue"] = sales["Revenue"].str.strip("$")
  sales["Revenue"] = sales["Revenue"].astype("int")
```

To make sure that everything worked, we can use the `assert` statement. `assert` takes
in a condition as input, and returns nothing if the condition is met, else it returns
an error.

```python
  assert sales["Revenue"].dtype == "int"
```

> #### More on `assert`
>
> To illustrate how this works, let us look at two simple examples:
>
> EXAMPLE 1
>
> ```python
>   assert 1 + 1 == 2
> ```
>
> This passes, because the condition is indeed True! So nothing will be returned and
> life continues as per normal.
>
> EXAMPLE 2
>
> ```python
>   assert 1 + 1 == 3
> ```
>
> ```console
>   AssertionError
>            assert 1 + 1 == 3
>   AssertionError:
> ```
>
> We can use `assert` to test practically any condition.

### Example: Numeric or Categorical

Sometimes, numbers can be used to represent categories (categorical data).

For example, say we have a column `marriage_status` in a DataFrame `df`.
We use numbers to represent the respective statuses - `0`: Never married; `1`: Married;
`2`: Separated; `3`: Divorced.

```console
   ...    marriage_status    ...
   ...                  3    ...
   ...                  1    ...
   ...                  2    ...
```

However, if we import this column, it will be imported as integers, which leads to
misleading results when trying to get stats.

```python
  df["marriage_status"].describe()
```

```console
          marriage_status
  ...
  mean               1.4
  std                0.20
  min                0.00
  50%                1.8 ...
```

To resolve this, we use the `.astype()` method (again) to typecast the data into
`category` from `int`.

```python
  df["marriage_status"] = df["marriage_status"].astype("category")
  df.describe()
```

Now, when we use the `.describe()` method, we can see that the statistical data are
more aligned with what you would expect out of categorical data.

```console
            marriage_status
  count                   241
  unique                  4
  top                     1
  freq                    120
```

## Data Range Constraints

Sometimes, we expect data to fall within a certain range. For example, surveys that ask
respondents to vote a score between 1 and 5. If your collected data shows 6, that's weird,
right? You should not be expecting that! This can be due to errors in data collection
or other factors.

### Dealing with out-of-range data

So, how exactly do we deal with out-of-range data?

1. Dropping data

If the dataset is small enough, we can consider dropping data points that have out-of-range
data. However, if there is too much out-of-range data, we may lose out on essential
information. Thus, only drop data if only a small proportion of data is affected.

2. Setting custom minimums and maximums

This allows you to maintain a specific range that you desire.

3. Treat out-of-range data as missing and impute

4. Assign custom value depending on business assumptions

We assign custom values for data that go beyond our range.

### Example: Movie Ratings

We start with a Pandas DataFrame `movies`, and the movies are rated on a scale of 1 to
5. Ratings are stored in a column called `avg_rating`. However, upon closer
inspection, we found ratings that were strangely above 5!

```python
  import pandas as pd
  movies[movies["avg_rating"] > 5]
```

```console
        movie_name  avg_rating
  25  I want to cry           6
  29  Bleh blessing           6
```

#### Dropping Data

Since there is only a small proportion of data affected, we can try dropping the data.

The data can be dropped in two ways:

1. create a new filtered `movies` DataFrame where `movies` now only contain
movies with ratings between 1 and 5.

```python
  movies = movies[movies["avg_rating"] <= 5]
```

2. drop the values using the `.drop()` method. This method takes in the *row*
*indices* of `movies` for which `avg_rating` is higher than 5 as the argument.

```python
  movies.drop(movies[movies["avg_rating"] > 5].index, inplace=True)
  ```

We set `inplace` to be True so that values are dropped in place and we do not have to
create a new column. This can be checked by using an `assert` statement.

```python
  assert movies["avg_rating"].max() <= 5
```

This statement should not return anything if data dropping was done correctly.

#### Custom Limits

Depending on the assumptions behind our data, we can also change the out-of-range data
to a hard limit. For example, we can set all average ratings above 5 as 5.

```python
  movies.loc[movies["avg_rating"] > 5, "avg_rating"] = 5
```

Here, we used the `.loc()` method, which we touched on in previous modules.

To check if this process worked, we can use the same `assert` statement as before. Neat!

### Example: Subscription Dates

We have a DataFrame `user_signups`, which stores the data of users subscribed to
PirateFlix (hehe).

First, we want to check the data types of data stored in the columns of
`user_signups`.

```python
  import datetime as dt
  import pandas as pd

  user_signups.dtypes
```

```console
  subscription_date    object
  user_name            object
  Country              object
  dtype: object
```

First, we can observe that `user_signups` has three columns. The one we are
interested in, `subscription_date`, has its data stored as an object, and not a date
or datetime object.

To compare a Pandas object to dates, we first need to convert the data into dates.

```python
  user_signups["subscription_date"] = pd.to_datetime(user_signups["subscription_date"]).dt.date
```

To do this, we first convert the object into a Pandas datetime object using the
`pd.to_datetime()` function, which takes in the column we want to convert. We then
convert the Pandas datetime object to a date via `.dt.date`.

Alternatively, we could have directly converted from the Pandas object itself
without the `to_datetime()` function, but we would have needed to provide
information about the date's format as a string.

Now that we have the data typecasted as datetime objects, we can do our job now!

We use today's date as our reference point (since you can't subscribe for a service
with a date later than today).

```python
  today_date = dt.date.today()
```

Similar to our previous example, we can either drop out of range data points or
hardcover them with custom limits.

#### Dropping Data

We will drop all dates that are later than `today_date`.

```python
  # drop by Filtering
  user_signups = user_signups[user_signups["subscription_date"] <= today_date]

  # drop values using .drop()
  user_signups.drop(user_signups[["subscription_date"] > today_date].index, inplace = True)
```

We can then use an `assert` statement to check if the data is dropped.

```python
  assert user_signups["subscription_date"].max().date() <= today_date
```

Small note: we have to add the `.date()` method to ensure that we get a date, not
a timestamp object.

#### Hardcode dates with upper limit

We will change all dates after today to today's date.

```python
  user_signups.loc(user_signups["subscription_date"] > today_date, "subscription_date") = today_date
```

The same assert statement above can be used to verify that all out-of-range values
have been edited.

## Unique Constraints/Duplicate Values

Duplicate values can be diagnosed when we have the same exact information repeated
across multiple rows in one or more columns.

Here are some reasons why they may happen:

1. Data Entry & Human Error

Sometimes, data might have been entered wrongly the first time, and a new entry is
created just to change some of the information, but the rest are the same.

2. Bugs and design errors

3. Join or Merge Errors

This is the most common one. Joining or merging databases is a necessary act to
consolidate data from various resources which could retain duplicate values.

### Finding duplicate values

We can find duplicates in a DataFrame by using the `.duplicated()` method. This
method returns a Series, with each row being `True` if it is duplicated, and
`False` if it is unique.

By default, all values are set as `True` except for the first occurrence of each row.

This however only works if you get duplicates across all columns.

If we do the following, we can see which rows are affected. Assume we have a DataFrame
`df`:

```python
  import pandas as pd
  duplicates = df.duplicated()
  df[duplicated]
```

Remember how we used Series of booleans to filter DataFrames? It is exactly the same!

If we do not customise the arguments of `.duplicated()`, duplicate rows will only
be picked out if entire rows are repeated. This limits our ability to diagnose
duplicate values/types, and how to treat them.

To calibrate how we find duplicates, we use 2 arguments from `.duplicated()`:

1. `subset`: the list of column names to check for duplication. For example, it
allows us to check for duplicates in the first and last columns only.

2. `keep`: whether to keep *first* (`"first"`), *last* (`"last"`) or *all*
(`False`) duplicate values

We can use `.sort_values(by=col_name)` to sort the data to see duplicate values
clearly.

**Useful Tip:** Subsetting on metadata (the information that describes and explains data,
like names, source, type, owner and relationships to other data sets) gives you a better
bird's eye view over the data and how to identify duplicates! 

### Treating duplicate values

We can classify duplicates into 2 broad categories:

1. complete duplicates: all data in the rows are entirely the same

These are easier to treat. All that is required is just to keep one of the rows
and discard the others.

This can be done with the `.drop_duplicates()` method. It takes in the
following arguments:

- `subset`: the list of column names to check for duplication. For example, it
allows us to check for duplicates in the first and last columns only.

- `keep`: whether to keep *first* (`"first"`), *last* (`"last"`) or *all*
(`False`) duplicate values

- `inplace`: Drop duplicate rows directly inside DataFrame without creating new
object (`True`)

2. incomplete duplicates: some data in the rows are different, while some are
duplicated

Apart from dropping rows with really small discrepancies, we can use statistical
measures to combine each set of duplicated data. Examples include taking mean or
maximum. This depends on a common sense understanding of the data -
what is suitable?

We can easily do this using the `.groupby()` method, which when chained
with the `.agg()` method, lets us group a set of common columns and return
statistical values for specific columns when the aggregation is being performed.

For example, say we have a DataFrame `df` that stores the height and weight of
people. `height` and `weight` are columns in `df`.

To group duplicate values together, we created a dictionary `summary`, that
instructs `.groupby()` to return the maximum of duplicated rows for `height`,
and the mean duplicated rows for `weight`.

```python
  col_names = ["first_name", "last_name", "address"]
  summary = {"height": "max", "weight": "mean"}
  df = df.groupby(by=col_name).agg(summary).reset_index()
```

We grouped `df` by the column names defined in `col_name` and chained it with
the `.agg()` method, which takes in `summary`. Finally, we add the
`.reset_index()` method at the end so we can have numbered indices in the final
output.

To ensure there are no longer duplicated values present, we can use the
`.duplicated()` method to generate a Series of booleans to output duplicated
rows from `df`.

```python
  duplicates = df.duplicated(subset=col_names, keep=False)
  df[duplicates].sort_values(by="first_name")
```

```console
  first_name    last_name    address    height    weight
```

As we can see, there aren't anymore duplicate rows present!
