# Common Data Problems

## Why clean data?

In a typical data science workflow, we will:

1. access raw data 
2. explore and process data
3. extract insights using visualisations or predicted models
4. report insights with dashboards or reports

Dirty data can appear because of duplicate values, misspellings, data type parsing errors
and legacy systems. (There are more reasons, really.)

If we do not make sure that data is clean before our workflow starts, it will definitely
affect the eventual final product that we produce (ie the insights).

## Data Type Constraints

There are many types of data we work with! This table shows the more common data types
used.

| Datatypes | Python Data Types |
|---|---|
| Text data | `str` |
| Integers | `int` |
| Decimals | `float` |
| Binary | `bool` |
| Date | `datetime` |
| Categories | `category` |

To make sure our analyses make sense, we need to make sure our data are represented
by the correct datatypes.

### Example: Strings to Integers

For example, say we have a csv file that contains the revenue and quantity of each
sales order, `sales.csv`.

We convert it into a DataFrame and output the head:

```python
  import python as pd
  sales = pd.read_csv("sales.csv")
  sales.head(2)
```

```console
    SalesOrderID    Revenue    Quantity
  0        43659     23153$          12
  1        43660      1457$           2
```

We want to calculate the total revenue generated by all sales orders.

We can see that each entry in the column `Revenue` has `$` on the right hand side.
This suggests that entries under `Revenue` are likely strings.

To further confirm our suspicions, we use the `.dtypes` method to inspect the data types
of each column.

```python
  sales.dtypes
```

```console
  SalesOrderID    int64
  Revenue         object
  Quantity        int64
  dtype: object
```

`object` - used to store strings in Pandas - is returned for the `Revenue` column,
confirming our suspicions!

> Alternatively, we could have used the `.info()` method too. This retrieves
> DataFrame information, which includes not just the data types, but also the number
> of missing values in the data.

If we summed up all entries in the `Revenue` column directly, it will result in
a concatenation of all the texts since they are all strings!

```python
  sales["Revenue"].sum()
```

```console
'23153$1457$36865$...
```

Thus, we have to first process the data.

We do this by removing `$` at the end of each entry using the `.strip()` method and
pass in the string we want to strip as the argument (in this case `$`).

Then, we typecast each entry from `str` to `int` using the `.astype()` method. Within the
method, we specify our desired type as the argument. This means it could have easily been
`"float"` being passed in, and each entry will be typecasted into decimal values instead
of integers.

```python
  sales["Revenue"] = sales["Revenue"].str.strip("$")
  sales["Revenue"] = sales["Revenue"].astype("int")
```

To make sure that everything worked, we can use the `assert` statement. `assert` takes
in a condition as input, and returns nothing if the condition is met, else it returns
an error.

```python
  assert sales["Revenue"].dtype == "int"
```

> #### More on `assert`
>
> To illustrate how this works, let us look at two simple examples:
>
> EXAMPLE 1
>
> ```python
>   assert 1 + 1 == 2
> ```
>
> This passes, because the condition is indeed True! So nothing will be returned and
> life continues as per normal.
>
> EXAMPLE 2
>
> ```python
>   assert 1 + 1 == 3
> ```
>
> ```console
>   AssertionError
>            assert 1 + 1 == 3
>   AssertionError:
> ```
>
> We can use `assert` to test practically any condition.

### Example: Numeric or Categorical

Sometimes, numbers can be used to represent categories (categorical data).

For example, say we have a column `marriage_status` in a DataFrame `df`.
We use numbers to represent the respective statuses - `0`: Never married; `1`: Married;
`2`: Separated; `3`: Divorced.

```console
   ...    marriage_status    ...
   ...                  3    ...
   ...                  1    ...
   ...                  2    ...
```

However, if we import this column, it will be imported as integers, which leads to
misleading results when trying to get stats.

```python
  df["marriage_status"].describe()
```

```console
          marriage_status
  ...
  mean               1.4
  std                0.20
  min                0.00
  50%                1.8 ...
```

To resolve this, we use the `.astype()` method (again) to typecast the data into
`category` from `int`.

```python
  df["marriage_status"] = df["marriage_status"].astype("category")
  df.describe()
```

Now, when we use the `.describe()` method, we can see that the statistical data are
more aligned with what you would expect out of categorical data.

```console
            marriage_status
  count                   241
  unique                  4
  top                     1
  freq                    120
```

## Data Range Constraints

Sometimes, we expect data to fall within a certain range. For example, surveys that ask
respondents to vote a score between 1 and 5. If your collected data shows 6, that's weird
right? You should not be expecting that! This can be due to errors in data collection
or other factors.

### Dealing with out of range data

So, how exactly do we deal with out of range data?

1. Dropping data

If the dataset is small enough, we can consider dropping data points that have out
of range data. However, if there is too much out of range data, we may lose out on
essential information. Thus, only drop data if only a small proportion of data is
affected.

2. Setting custom minimums and maximums

This allows you to maintain a specific range that you desire.

3. Treat out of range data as missing and impute

4. Assign custom value depending on business assumptions

We assign custom values for data that go beyond our range.

### Example: Movie Ratings

We start with a Pandas DataFrame `movies`, and the movies are rated on a scale of 1 to
5. Ratings are stored in a column called `avg_rating`. However, upon closer
inspection, we found ratings that were strangely above 5!

```python
  import pandas as pd
  movies[movies["avg_rating"] > 5]
```

```console
        movie_name  avg_rating
  25  I want to cry           6
  29  Bleh blessing           6
```

Since there is only a small proportion of data affected, we can try dropping the data.

The data can be dropped in two ways:

1. create a new filtered `movies` DataFrame where `movies` now only contain
movies with rating between 1 and 5.

```python
  movies = movies[movies["avg_rating"] <= 5]
```

2. drop the values using the `.drop()` method. This method takes in the *row*
*indices* of `movies` for which `avg_rating` is higher than 5 as the argument.

```python
  movies.drop(movies[movies["avg_rating"] > 5].index, inplace=True)
  ```

We set `inplace` to be True so that values are dropped in place and we do not have to
create a new column. This can be checked by using an `assert` statement.

```python
  assert movies["avg_rating"].max() <= 5